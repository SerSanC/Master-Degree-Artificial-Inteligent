{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SerSanC/Master-Degree-Artificial-Inteligent/blob/main/ProjectSupervisedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4tQ26lwD9GT"
      },
      "source": [
        "# 1.- Preprocessing\n",
        "---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRylKtVMSnK2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "import random\n",
        "import shutil\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u7Avt50TBFp"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpu4Dlq-SwOE"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Datos/Sergi/eyes'\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                 transforms.CenterCrop(224),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                     (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "dataset = datasets.ImageFolder(path,transform=transform)\n",
        "classes = dataset.classes\n",
        "data_loader = DataLoader(dataset, batch_size=20, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS8h2-aXqjyG"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5 \n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "\n",
        "dataiter = iter(data_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "fig = plt.figure(figsize=(20, 4))\n",
        "\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9CURjk6RJ9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_set, test_set = random_split(dataset, (int(len(dataset) * 0.7) + 1, int(len(dataset) * 0.3)))\n",
        "train_set, valid_set = random_split(train_set, (int(len(train_set) * 0.7) + 1, int(len(train_set) * 0.3)))\n",
        "\n",
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Vhcpr4oLW3"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=64)\n",
        "valid_loader = DataLoader(valid_set, batch_size=1)\n",
        "test_loader = DataLoader(test_set, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.- CNN"
      ],
      "metadata": {
        "id": "LHtFiboDQKkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6VPF6l_oP5g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(46656, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j89t4qx-qPha"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cnn = Net().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# params = cnn.resnet.fc.parameters()\n",
        "params = cnn.parameters()\n",
        "optimizer = Adam(params, lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohiZ6oZKqUUz"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, device):\n",
        "  total_step = len(train_loader)\n",
        "  num_epochs = 10\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for i, (img, target) in enumerate(train_loader):\n",
        "      img = img.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(img)\n",
        "      \n",
        "\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item() * img.size(0)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      valid_loss += loss.item() * data.size(0)\n",
        "    train_loss = train_loss / len(train_loader.sampler)\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "          epoch, train_loss, valid_loss))\n",
        "\n",
        "  print(f\"Training Losses: {train_losses}\")  \n",
        "  print(f\"Valid Losses: {valid_losses}\")  \n",
        "  plt.plot(train_losses, label='Training loss')\n",
        "  plt.plot(valid_losses, label='Validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RsTPBkvqYOs"
      },
      "outputs": [],
      "source": [
        "train_model(cnn, train_loader, valid_loader, criterion, optimizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po8mYVgDqbGX"
      },
      "outputs": [],
      "source": [
        "def global_accuracy(model, test_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.to(\"cpu\")\n",
        "  dataiter = iter(test_loader)\n",
        "  with torch.no_grad():\n",
        "    for data in dataiter:\n",
        "      img, label = data\n",
        "      output = model(img)\n",
        "      _, predicted = torch.max(output.data, 1)\n",
        "      total += label.size(0)\n",
        "      correct += (predicted == label).sum().item()\n",
        "\n",
        "  print(f\"Accuracy: {100 * correct / total}\")\n",
        "\n",
        "def accuracy_per_class(model, test_loader, classes, device):\n",
        "  class_correct = list(0. for i in range(5))\n",
        "  class_total = list(0. for i in range(5))\n",
        "  cnn.to(device)\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          images, labels = data\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = cnn(images)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          c = (predicted == labels).squeeze()\n",
        "          if(c.item()):\n",
        "            class_correct[labels.item()] += 1\n",
        "          class_total[labels.item()] += 1\n",
        "  \n",
        "\n",
        "  accuracy = 0\n",
        "  for i in range(0,2):\n",
        "    print\n",
        "    if int(class_total[i]) == 0:\n",
        "      accuracy = 0\n",
        "    else:\n",
        "      accuracy = float(class_correct[i] / class_total[i])\n",
        "    print(f\"{classes[i]} | Correct: {class_correct[i]} | Total: {class_total[i]}\" +\n",
        "          f\" | Accuracy: {accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2tN16lqj-OS"
      },
      "outputs": [],
      "source": [
        "global_accuracy(cnn, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyh4vxCFqfqD"
      },
      "outputs": [],
      "source": [
        "accuracy_per_class(cnn, test_loader, classes, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfacDGWCIio7"
      },
      "outputs": [],
      "source": [
        "torch.save(cnn.state_dict(), \"cnn_basic.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6keHtMwLImqe"
      },
      "outputs": [],
      "source": [
        "model = Net()\n",
        "model.load_state_dict(torch.load('cnn_basic.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhp52cBijNGb"
      },
      "outputs": [],
      "source": [
        "from torchvision import models,transforms\n",
        "\n",
        "class ClassifierResnet34(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ClassifierResnet34, self).__init__()\n",
        "    self.resnet = models.resnet34(pretrained=True)\n",
        "    self.resnet.fc = nn.Linear(self.resnet.fc.in_features,5)\n",
        "    \n",
        "\n",
        "  def forward(self, image):\n",
        "    output = self.resnet(image)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0brtXCwdINHN"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "cnn = ClassifierResnet34().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = cnn.resnet.fc.parameters()\n",
        "optimizer = Adam(params, lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(cnn, train_loader, valid_loader, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "TwVznPunUCu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_accuracy(cnn, test_loader)\n",
        "accuracy_per_class(cnn, test_loader, classes, device)"
      ],
      "metadata": {
        "id": "dWpUMjjVUE5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models,transforms\n",
        "\n",
        "class ClassifierResnet50(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ClassifierResnet50, self).__init__()\n",
        "    self.resnet = models.resnet50(pretrained=True)\n",
        "    self.resnet.fc = nn.Linear(self.resnet.fc.in_features,5)\n",
        "    \n",
        "\n",
        "  def forward(self, image):\n",
        "    output = self.resnet(image)\n",
        "    return output"
      ],
      "metadata": {
        "id": "dfkwh4itQ41k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "cnn = ClassifierResnet50().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = cnn.resnet.fc.parameters()\n",
        "optimizer = Adam(params, lr=0.003)"
      ],
      "metadata": {
        "id": "lUmM_DrXUI-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(cnn, train_loader, valid_loader, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "bVmuRVPYUMeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_accuracy(cnn, test_loader)\n",
        "accuracy_per_class(cnn, test_loader, classes, device)"
      ],
      "metadata": {
        "id": "xwwvF0RHufGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "# Experimentos"
      ],
      "metadata": {
        "id": "penZ4RugOFzU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snJwIqCGXZcz"
      },
      "outputs": [],
      "source": [
        "from torchvision import models,transforms\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(25088, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv5_1(x))\n",
        "        x = F.relu(self.conv5_2(x))\n",
        "        x = F.relu(self.conv5_3(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, 0.5) #dropout was included to combat overfitting\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, 0.5)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyj2wjXrYCg0"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam,SGD\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #training with either cpu or cuda\n",
        "\n",
        "model = VGG16() #to compile the model\n",
        "model = model.to(device=device) #to send the model for training on either cuda or cpu\n",
        "\n",
        "## Loss and optimizer\n",
        "learning_rate = 1e-4 #I picked this because it seems to be the most used by experts\n",
        "load_model = True\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr= learning_rate) #Adam seems to be the most popular for deep learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, valid_loader, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "feJB56blukUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_accuracy(model, test_loader)\n",
        "accuracy_per_class(model, test_loader, classes, device)"
      ],
      "metadata": {
        "id": "u9TSop-VuhBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam,SGD\n",
        "\n",
        "## Load the model based on VGG19\n",
        "vgg_based = torchvision.models.vgg19(pretrained=True)\n",
        "\n",
        "## freeze the layers\n",
        "for param in vgg_based.parameters():\n",
        "   param.requires_grad = False\n",
        "\n",
        "# Modify the last layer\n",
        "number_features = vgg_based.classifier[6].in_features\n",
        "features = list(vgg_based.classifier.children())[:-1] # Remove last layer\n",
        "features.extend([torch.nn.Linear(number_features, len(classes))])\n",
        "vgg_based.classifier = torch.nn.Sequential(*features)\n",
        "\n",
        "vgg_based = vgg_based.to(device)\n",
        "\n",
        "print(vgg_based)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer_ft = SGD(vgg_based.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "Zv1DdH_3-RPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(vgg_based, train_loader, valid_loader, criterion, optimizer_ft, device)"
      ],
      "metadata": {
        "id": "kRdsuiJj04te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_accuracy(vgg_based, test_loader)\n",
        "accuracy_per_class(vgg_based, test_loader, classes, device)"
      ],
      "metadata": {
        "id": "BvDpkjkm1TF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ProjectSupervisedLearning.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1kYoEukRIiKW6F74YU-vq1tWuGh-cc2Jl",
      "authorship_tag": "ABX9TyPf0tnbSC5ZMoOR1TB8GIWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}